# rag_ollama_faiss.py
from flask import Flask, request, jsonify
from flask_cors import CORS
from datetime import datetime, timezone
import os
import threading
import re
import faiss
import portalocker
import numpy as np
from langchain_ollama import OllamaEmbeddings, OllamaLLM

# ==================== é…ç½®åŠ è½½ ====================
app = Flask(__name__)
# CORS(app, resources={
#     r"/api/*": {"origins": ["http://localhost:3000"]},  # åªå…è®¸ React å‰ç«¯
#     r"/add": {"origins": ["http://localhost:3000"]},
#     r"/query": {"origins": ["http://localhost:3000"]},
#     r"/summary": {"origins": ["http://localhost:3000"]},
#     r"/health": {"origins": ["http://localhost:3000"]},
# })

DATA_DIR = "myRAG/data"
os.makedirs(DATA_DIR, exist_ok=True)
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
EXPENSES_FILE = os.path.join(DATA_DIR, "expense.txt")
INDEX_FILE = os.path.join(DATA_DIR, "faiss_index.bin")          # â† ç´¢å¼•æ–‡ä»¶
PROGRESS_FILE = os.path.join(DATA_DIR, "embedding_progress.txt")  # â† è¿›åº¦æ–‡ä»¶

print("EXPENSES_FILE:" , EXPENSES_FILE)
print("INDEX_FILE:"    , INDEX_FILE)
print("PROGRESS_FILE:" , PROGRESS_FILE)
# ================ 1. åˆå§‹åŒ– Ollama ç»„ä»¶ ================
# æ£€æŸ¥ Ollama æ˜¯å¦è¿è¡Œ
try:
    import requests
    assert requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=2).status_code == 200
except:
    raise RuntimeError("âŒ è¯·å…ˆå¯åŠ¨ Ollamaï¼šollama serve")

# Embedding æ¨¡å‹ï¼ˆnomic-embed-textï¼Œ768ç»´ï¼‰
embedding_model = OllamaEmbeddings(
    model="nomic-embed-text",
    base_url=OLLAMA_BASE_URL,
)

# LLM æ¨¡å‹ï¼ˆqwen:4bï¼Œä¸­æ–‡ä¼˜åŒ–ï¼‰
llm = OllamaLLM(
    model="qwen3:4b",
    base_url=OLLAMA_BASE_URL,
    temperature=0.3,
    stop=["\n\n"],  # â† å…³é”®ï¼é˜²è¿‡æ—©
    keep_alive="5m",    # ä¿æŒæ¨¡å‹åŠ è½½
    stream=False,       # å¼ºåˆ¶éæµå¼
)

def add_expense(raw_text: str):

    # å¸¸è§æ—¥æœŸæ ¼å¼æ­£åˆ™è¡¨è¾¾å¼
    date_patterns = [
        r'(\d{1,2}æœˆ\d{1,2}æ—¥)',            # 3æœˆ11æ—¥
        r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',     # 2024å¹´3æœˆ11æ—¥
        r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})',  # 2024-03-11 æˆ– 2024/03/11
        r'(\d{1,2}[-/]\d{1,2}[-/]\d{4})',  # 03-11-2024 æˆ– 03/11/2024
    ]
    
    # æŸ¥æ‰¾æ—¥æœŸ
    found_date = None
    for pattern in date_patterns:
        match = re.search(pattern, raw_text)
        if match:
            date_str = match.group(1)
            try:
                # å°è¯•è§£ææ—¥æœŸ
                if 'å¹´' in date_str:
                    date = datetime.strptime(date_str, "%Yå¹´%mæœˆ%dæ—¥")
                elif 'æœˆ' in date_str:
                    # å¤„ç†"3æœˆ11æ—¥"æ ¼å¼
                    current_year = datetime.now().year
                    date = datetime.strptime(f"{current_year}å¹´{date_str}", "%Yå¹´%mæœˆ%dæ—¥")
                elif '/' in date_str or '-' in date_str:
                    parts = re.split('[-/]', date_str)
                    if len(parts[0]) == 4:  # 2024-03-11
                        date = datetime.strptime(date_str, "%Y-%m-%d")
                    else:  # 03-11-2024
                        date = datetime.strptime(date_str, "%m-%d-%Y")
                else:
                    continue
                
                # è½¬æ¢ä¸ºå¸¦æ—¶åŒºçš„datetimeå¯¹è±¡
                found_date = date.replace(tzinfo=timezone.utc).astimezone()
                # ä»åŸå§‹æ–‡æœ¬ä¸­ç§»é™¤æ—¥æœŸéƒ¨åˆ†
                raw_text = raw_text.replace(date_str, "").strip()
                break
            except ValueError as e:
                print(f"æ—¥æœŸè§£æé”™è¯¯: {e}")
                continue

    # ç”Ÿæˆ ISO 8601 æ—¶é—´æˆ³ï¼ˆå¸¦æ—¶åŒºï¼‰
    now = found_date if found_date else datetime.now(timezone.utc).astimezone()  # æœ¬åœ°æ—¶åŒº
    timestamp = now.isoformat(timespec='seconds')  # e.g., 2025-12-25T08:30:12+08:00
    line = f"{timestamp} | {raw_text.strip()}\n"
    
    # åŸå­è¿½åŠ åˆ° expenses.txt
    try:
        with open(EXPENSES_FILE, "a", encoding="utf-8") as f:
            portalocker.lock(f, portalocker.LOCK_EX)
            f.write(line)
            f.flush()
            os.fsync(f.fileno())
        print(f"âœ… å·²è®°è´¦: {line.strip()}")
        return True
    except IOError as e:
        print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")
        return False

# ==================== åŠ è½½/åˆ›å»º FAISS ç´¢å¼• ====================
def load_or_create_index():
    dim = 768
    # æƒ…å†µ 1: ç´¢å¼•æ–‡ä»¶å­˜åœ¨ â†’ ç›´æ¥åŠ è½½
    if os.path.exists(INDEX_FILE):
        index = faiss.read_index(INDEX_FILE)
        print(f"âœ… åŠ è½½ FAISS ç´¢å¼•ï¼Œæ¡ç›®æ•°: {index.ntotal}")
        return index

    # æƒ…å†µ 2: ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ â†’ æ£€æŸ¥è´¦æœ¬æ˜¯å¦ä¸ºç©º
    if not os.path.exists(EXPENSES_FILE):
        open(EXPENSES_FILE, "w", encoding="utf-8").close()
    
    with open(EXPENSES_FILE, "r", encoding="utf-8") as f:
        all_lines = [line for line in f if line.strip() and not line.startswith("#")]
    
    if not all_lines:
        # è´¦æœ¬ä¹Ÿä¸ºç©º â†’ æ–°å»ºç©ºç´¢å¼•   
        index = faiss.IndexIDMap(faiss.IndexFlatIP(dim))
        faiss.normalize_L2(np.zeros((1, dim), dtype="float32"))
        print("ğŸ†• åˆ›å»ºç©º FAISS ç´¢å¼•ï¼ˆé¦–æ¬¡å¯åŠ¨ï¼‰")
        return index

    # æƒ…å†µ 3: è´¦æœ¬æœ‰æ•°æ®ä½†ç´¢å¼•ä¸¢å¤± â†’ è‡ªåŠ¨é‡å»º
    print(f"âš ï¸ ç´¢å¼•æ–‡ä»¶ç¼ºå¤±ï¼Œä½†è´¦æœ¬æœ‰ {len(all_lines)} æ¡è®°å½•ï¼Œæ­£åœ¨é‡å»º...")
    index = faiss.IndexIDMap(faiss.IndexFlatIP(dim))
    faiss.normalize_L2(np.zeros((1, dim), dtype="float32"))
    
    # ä»å¤´å¼€å§‹å¤„ç†æ‰€æœ‰è´¦å•ï¼ˆæ¨¡æ‹Ÿé¦–æ¬¡å…¨é‡ï¼‰
    texts, ids = [], []
    for i, line in enumerate(all_lines, start=1):
        try:
            parts = line.strip().split("|", 1)
            if len(parts) >= 2:
                texts.append(parts[1].strip())
                ids.append(i)
        except Exception as e:
            print(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ {i}: {e}")
            continue

    if texts:
        vectors = np.array(embedding_model.embed_documents(texts)).astype("float32")
        faiss.normalize_L2(vectors)
        ids_arr = np.array(ids, dtype=np.int64)
        index.add_with_ids(vectors, ids_arr)
        print(f"âœ… é‡å»ºç´¢å¼•æˆåŠŸï¼Œå·²å¤„ç† {len(ids)} æ¡è´¦å•")
    else:
        print("âš ï¸ è´¦æœ¬ä¸­æ— æœ‰æ•ˆæ–‡æœ¬ï¼Œåˆ›å»ºç©ºç´¢å¼•")
    
    # ä¿å­˜é‡å»ºåçš„ç´¢å¼•
    faiss.write_index(index, INDEX_FILE)
    save_progress(len(all_lines), "nomic-embed-text@20251229")  # æ›´æ–°è¿›åº¦
    return index

# ==================== è¿›åº¦ç®¡ç† ====================
def load_progress():
    if not os.path.exists(PROGRESS_FILE):
        return 0, "nomic-embed-text@20251229"
    with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f if line.strip()]
        last_line = int(lines[0].split("=")[1]) if len(lines) > 0 else 0
        version = lines[1].split("=")[1] if len(lines) > 1 else "unknown"
    return last_line, version

def save_progress(last_line: int, version: str):
    with open(PROGRESS_FILE, "w", encoding="utf-8") as f:
        f.write(f"last_processed_line = {last_line}\n")
        f.write(f"embedding_model_version = {version}\n")

def update_embeddings_incremental():
    # 1. åŠ è½½ç´¢å¼• & è¿›åº¦
    index = load_or_create_index()  # ç”¨ä½ å‰é¢å®šä¹‰çš„å‡½æ•°
    last_line, current_version = load_progress()
    
    # è¯»å–æ‰€æœ‰è´¦å•è¡Œ
    if not os.path.exists(EXPENSES_FILE):
        open(EXPENSES_FILE, "w", encoding="utf-8").close()
    with open(EXPENSES_FILE, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    total_lines = len(lines)
    new_lines = lines[last_line:]  # ä» last_line å¼€å§‹ï¼ˆ0-indexed â†’ line 1 æ˜¯ index 0ï¼‰
    
    if not new_lines:
        print("â­ï¸ æ— æ–°å¢è´¦å•")
        return
    
    print(f"ğŸ”„ å‘ç° {len(new_lines)} æ¡æ–°è´¦å•ï¼ˆè¡Œ {last_line+1} ~ {total_lines}ï¼‰")

    # 3. æå– raw_text å¹¶ç”Ÿæˆ embedding
    texts, new_ids = [], []
    for i, line in enumerate(new_lines, start=last_line+1):
        if line.strip() and not line.startswith("#"):
            try:
                # è§£æï¼š "timestamp | text" â†’ å– | åéƒ¨åˆ†
                parts = line.strip().split("|", 1)
                if len(parts) >= 2:
                    raw_text = parts[1].strip()
                    texts.append(raw_text)
                    new_ids.append(i)  # è¡Œå·ä½œä¸º ID
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ {i}: {e}")
                continue
    
    if not texts:
        print("â­ï¸ æ— æœ‰æ•ˆæ–°è´¦å•")
        return

    vectors = np.array(embedding_model.embed_documents(texts)).astype("float32")
    faiss.normalize_L2(vectors)

    # 4. å¢é‡åŠ å…¥ FAISS
    ids_arr = np.array(new_ids, dtype=np.int64)
    index.add_with_ids(vectors, ids_arr)

    # 5. æŒä¹…åŒ–
    faiss.write_index(index, INDEX_FILE)
    save_progress(total_lines, current_version)

    print(f"âœ… æ–°å¢ {len(new_ids)} æ¡ embeddingï¼Œç´¢å¼•å·²ä¿å­˜")

# ==================== æŒ‰ ID è·å–åŸæ–‡ï¼ˆæ›¿ä»£ chunksï¼‰ ====================
def get_context_by_ids(ids: list) -> str:
    """æ ¹æ®è¡Œå·åˆ—è¡¨ï¼Œä» expenses.txt è¯»å–åŸæ–‡"""
    if not ids:
        return ""
    with open(EXPENSES_FILE, "r", encoding="utf-8") as f:
        lines = f.readlines()
    context_parts = []
    for i, line_id in enumerate(ids):
        if 1 <= line_id <= len(lines):
            line = lines[line_id - 1].strip()
            if line and not line.startswith("#"):
                # æå– raw_text éƒ¨åˆ†
                parts = line.split("|", 1)
                text = parts[1].strip() if len(parts) > 1 else line
                context_parts.append(f"[{i+1}] {text} ï¼ˆ{parts[0].strip()}ï¼‰")
    return "\n".join(context_parts)

# ================ RAG æ¨ç†å‡½æ•° ================
def rag_query(question: str, k: int = 3) -> str:
    index = load_or_create_index()
    if index.ntotal == 0:
        return "ğŸ“¦ å½“å‰æ— è´¦å•ï¼Œè¯·å…ˆè®°è´¦ã€‚"
    
    # 1. ç”Ÿæˆ query embedding
    query_vec = embedding_model.embed_query(question)
    print("ç»´åº¦æ•°åº”ä¸º 768ï¼š", len(query_vec))
    query_vec = np.array(query_vec).astype("float32").reshape(1, -1)
    faiss.normalize_L2(query_vec)

    # 2. æ£€ç´¢ Top-K
    scores, indices = index.search(query_vec, k * 5)
    print("Top scores:", scores)
    print("Top indices:", indices)
    
    # 3. æ‹¼æ¥ä¸Šä¸‹æ–‡
    valid_ids = [int(idx) for idx in indices[0] if idx != -1]
    print("æœ‰æ•ˆ ID:", valid_ids)
    context = get_context_by_ids(valid_ids)
    if not context:
        return "ğŸ” æœªæ‰¾åˆ°ç›¸å…³è´¦å•ã€‚"

    # 4. æ„é€  prompt & ç”Ÿæˆ
    prompt = f"""ä½ æ˜¯ä¸€åèµ„æ·±ä¸ªäººè´¢åŠ¡ç®¡ç†å¸ˆï¼Œä»Šå¤©æ˜¯ {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")}ï¼Œè¯·ç»“åˆå½“å‰æ—¥æœŸå’Œä¸Šä¸‹æ–‡å›ç­”ä¸“ä¸šã€ç®€æ´åœ°å›ç­”é—®é¢˜ã€‚
è‹¥ä¸Šä¸‹æ–‡ä¸è¶³ï¼Œè¯·å›ç­”â€œæ ¹æ®ç°æœ‰èµ„æ–™æ— æ³•å›ç­”â€ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}
å›ç­”ï¼š"""
    
    print("Prompt:", prompt)
    return llm.invoke(prompt,options={"num_predict": 8192})

# ==================== Flask API è·¯ç”± ====================

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    try:
        # æ£€æŸ¥ Ollama æ˜¯å¦å¯è¾¾
        assert requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=2).status_code == 200
        # æ£€æŸ¥ FAISS ç´¢å¼•æ˜¯å¦åŠ è½½
        load_or_create_index()
        return jsonify({'status': 'healthy', 'timestamp': datetime.now().isoformat()}), 200
    except Exception as e:
        return jsonify({'status': 'unhealthy', 'error': str(e)}), 500
    
@app.route('/add', methods=['POST'])
def add_expense_api():
    """æ·»åŠ è´¦å•æ¥å£"""
    data = request.json
    raw_text = data.get('text', '').strip()
    
    if not raw_text:
        return jsonify({'error': 'text is required'}), 400
    
    success = add_expense(raw_text)
    if not success:
        return jsonify({'error': 'failed to write to file'}), 500

    # å¼‚æ­¥æ›´æ–° embeddingï¼ˆé¿å…é˜»å¡ APIï¼‰
    thread = threading.Thread(target=update_embeddings_incremental)
    thread.start()

    return jsonify({
        'message': 'expense added successfully',
        'timestamp': datetime.now().isoformat(),
        'text': raw_text
    }), 200

@app.route('/query', methods=['GET'])
def query_api():
    """è¯­ä¹‰æŸ¥è¯¢æ¥å£"""
    question = request.args.get('q', '').strip()
    if not question:
        return jsonify({'error': 'q parameter is required'}), 400

    try:
        answer = rag_query(question)
        return jsonify({
            'question': question,
            'answer': answer,
            'timestamp': datetime.now().isoformat()
        }), 200
    except Exception as e:
        return jsonify({'error': f'query failed: {str(e)}'}), 500
    
@app.route('/summary', methods=['GET'])
def summary_api():
    """æ—¶é—´èŒƒå›´æ±‡æ€»æ¥å£"""
    start_date = request.args.get('start')  # æ ¼å¼ï¼š2025-12-01
    end_date = request.args.get('end')      # æ ¼å¼ï¼š2025-12-31
    if not start_date or not end_date:
        return jsonify({'error': 'start and end date required (YYYY-MM-DD)'}), 400

    # è¯»å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„è´¦å•ï¼ˆç®€åŒ–ç‰ˆï¼šç›´æ¥è®© LLM åšæ±‡æ€»ï¼‰
    with open(EXPENSES_FILE, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    filtered_lines = []
    for line in lines:
        if line.strip() and not line.startswith("#"):
            try:
                ts_str = line.split("|", 1)[0].strip()
                ts = datetime.fromisoformat(ts_str)
                start_dt = datetime.fromisoformat(start_date)
                end_dt = datetime.fromisoformat(end_date)
                if start_dt.date() <= ts.date() <= end_dt.date():
                    filtered_lines.append(line.strip())
            except:
                continue

    if not filtered_lines:
        return jsonify({'summary': 'è¯¥æ—¶é—´æ®µå†…æ— è´¦å•è®°å½•'}), 200

    # è®© LLM åšæ±‡æ€»
    context = "\n".join(filtered_lines)
    prompt = f"""è¯·å¯¹ä»¥ä¸‹æ—¶é—´æ®µå†…çš„è´¦å•è¿›è¡Œåˆ†ç±»æ±‡æ€»ï¼š
æ—¶é—´æ®µï¼š{start_date} è‡³ {end_date}
è´¦å•è®°å½•ï¼š
{context}

è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š
1. æœ‰å“ªäº›æ¶ˆè´¹ç§ç±»ï¼Ÿ
2. å„ç§ç±»çš„æ€»é‡‘é¢æ˜¯å¤šå°‘ï¼Ÿ
3. å“ªäº›ç§ç±»å æ”¯å‡ºå¤§å¤´ï¼ˆå æ¯”æœ€é«˜ï¼‰ï¼Ÿ
è¯·æŒ‰æ¸…æ™°çš„æ ¼å¼å›ç­”ã€‚"""
    print("Prompt:", prompt)
    summary = llm.invoke(prompt,options={"num_predict": 8192})
    return jsonify({
        'summary': summary,
        'date_range': {'start': start_date, 'end': end_date},
        'total_records': len(filtered_lines)
    }), 200

@app.errorhandler(404)
def not_found(error):
    return jsonify({'error': 'endpoint not found'}), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({'error': 'internal server error'}), 500

# ==================== å¯åŠ¨å…¥å£ ====================
if __name__ == '__main__':
    print("ğŸŸ¢ Flask RAG API å¯åŠ¨ä¸­...")
    print(f"ğŸ“ æ•°æ®ç›®å½•: {DATA_DIR}")
    print(f"ğŸŒ Ollama åœ°å€: {OLLAMA_BASE_URL}")
    print("ğŸ’¡ API ç«¯ç‚¹:")
    print("   POST /add    -> æ·»åŠ è´¦å•")
    print("   GET  /query  -> è¯­ä¹‰æŸ¥è¯¢")
    print("   GET  /summary-> æ—¶é—´æ±‡æ€»")
    print("   GET  /health -> å¥åº·æ£€æŸ¥")
    
    # é¢„åŠ è½½ç´¢å¼•ï¼ˆé¿å…é¦–æ¬¡æŸ¥è¯¢æ…¢ï¼‰
    load_or_create_index()
    
    app.run(host='0.0.0.0', port=8911, debug=False)
